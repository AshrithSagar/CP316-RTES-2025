# RTES | 2025-03-18

## Inter-task data flow

- Synchronous data-processing
- Producer and consumer scenario
- Acuire and release
  - Wait and set
  - Critical region in between
  - Internally result in a state change
  - Call the scheduler

- Counting semaphores
- Topics
  - Scheduler
  - Context switch
  - Mutex / Semaphores
  - Flags / flag groups
    - Each flag is a group of bits
    - Combinations of them
- Producer and consumer
  - Producer fills up a buffer and sets a flag
  - Data sharing between the two
  - Problem: New data coming in while consuming
  - Scenario: Stream of data continuously comes in
  - Synchronous processing, say
  - Processing algorithm is deterministic
    - But it still takes some time, meanwhile new data can come in

### ~Synchronous data flows

#### Double buffering

- Adding another buffer is one solution

  Double buffer solution

- One is read into, one is being processed

- Switch the buffers when the slower entity has completed it's job

#### Ring buffer

- Another addition would be to use multiple buffers, and use a counting semaphore.

  - Processing times by CPU. Worst case and average case discrepancy.
  - Worst case is way off than average case, would require redundant storage allocation like these above.
  - Can be seen as one large buffer, divided into multiple parts

- One buffer is enough if lock-stepped

  - Say after processing, an acknowledgment signal is sent, and only then listen for new data.

- Eg: Scenario: `BASH` shell, typing while the shell is still booting up. You still need to listen to user input.

- Eg: Networking. Buffering streams.

- ```c
  // Producer
  while(1)
  {
    read; // From somewhere
    write(b); // Into the buffer
    set(f); // Set some flag
  }
  
  // Consumer
  while(1)
  {
    wait(f); // For the flag
    process(b); // the buffer data
  }
  ```

- The point is that producer needs to wait at some point of time.

- Another would be to create a new task with higher priority than the current one, and do producer operations there meanwhile.

- DMA

  - Direct memory access
  - Peripheral receives data and directly writes to the memory

- This is where message queues come in

### Asynchronous data flows

- Message queues

  - A semaphore + buffer is a message queue, in a way

  - Producer sends, consumer waits
- Variations are there in production and consumption rates
  - On average these two must match, else the system is not viable.
  - Supply / Demand scenario
- Message
  - Typically some ~data structure carrying some information
- Higher priority towards consumption, lower priority towards production

  - Outputs are real-time devices mostly
  - Eg: Can't stop playing music

#### Message queues in `CMSIS OS2`

- ```c
  mq_id = osMessageQueueNew(msg_count, msg_size, attr);
  osMessageQueuePut(mq_id, msg_ptr, msg_prio, timeout);
  osMessageQueueGet(mq_id, msg_ptr, &msg_prio, timeout);
  ```

- Number of buffers $\implies$ `msg_count`

- Fixed pre-allocation of buffer sizes; Considering worst-case size, say; In `msg_size`

- These are priority queues $\implies$ `msg_prio`

- Like a circular buffer implemented

  - The buffers are allocated to the message queue, i.e. pre-allocated buffers

- Good in terms of memory leaking protection

  - A challenging (and common) problem to solve

- Dynamic memory allocation not required, never need to do `malloc` etc here

  - No need to allocate any memory in the code

- Implicitly has a semaphore count? due to implementation as a linked list

  - Number of nodes in linked list known
  - Pointer is pointing to `null` or some message, and can determine based on that

- Is this abstraction any helpful?

  - In this case, memory wise, not much difference
  - Helps in read and write overflows between the buffers
    - Saves the pain to track read and write pointers and see whether they're overlapping, and determining buffer switch.
- Context switch both on queue put (queue full) and queue get (queue empty).
- Message memory allocation
  - Pass messages
    - Memory pre-allocated for `msg_size` $\times$ `msg_count`
  - Pass pointers
    - Another way, where the memory allocated is used to store a pointer ($\implies$ 4 bytes)
    - Producer allocates memory, and consumer reads the messages and frees it
    - ~ No need to copy data twice, also (~same system?)
- Designing the messages
  - In some way, say
  - Can pass length of buffer, etc
  - ~Application side, also need to comply accordingly
- Analogy
  - Message queue is a conveyer belt, with a pre-allocated length
  - Message is a bucket
  - Put buckets on the conveyer belt $\iff$ Passing messages
  - Puting hooks in the belt, and hang a bucket $\iff$ Passing pointers
- Memory leaking
  - One scenario is that,
  - Different times, different libraries have different internal assumptions
    - Discrepancies caused

- ~All this is linked list magic? :)

- `static` $\implies$ Not on stack, in ~global

- Why can't we use pointers always?

  - One problem is, if defined as local variables, then they're cleared later on, when going out of function context.

  - ```c
    void f_1() {
      char s[8];
      scanf("%s", s);
      queue_put(mq_id, s, msg_prio, timeout);
    }
    
    void t1() {
      while(1) {
        f_1();
        f_2();
        ...
      }
    }
    ```

  - In the above example, `s` would be cleared when going to `f_2`, since it's a local variable.

  - To avoid this, need to use `static` for `s`, maybe even pre-allocate an array of them and use

#### Memory pool in `CMSIS OS2`

- ```c
  mp_id = osMemoryPoolWindow(block_count, block_size, attr);
  ptr = osMemoryPoolAlloc(mp_id, timeout);
  osMessagePoolFree(mp_id, ptr);
  ```

- Stack vs queue

- Heaps

  - Initialised with a size
  - Can have different allocations, based on sizes
  - Analogy: Having separate parking lots for trucks, cars, etc

  - `FreeRTOS` has 5 heaps?

- Comparision with `malloc`
  - `malloc` allows requesting any dynamic size of memory
  - One issue with it is fragmentation
  - Patches of freed up memory exist, and won't have a contiguous memory.

#### Dynamic memory management

- Run-time allocation and freeing of memory
- `malloc` / `free`
  - A library function
  - May not be re-entrant, hence has mutual  exclusion issues
  - One heap for all sizes $\implies$ smaller blocks would create fragmentation
  - Fails and returns when heap is full
  - Also, not atomic, depending on which ~compiler
- Memory pools
  - An OS function
  - Implemented as an OS call, hence is atomic
  - Multiple pools of different sizes to avoid small blocks fragmenting memory
  - Allocation call waits if pool is full

## Timers

- Aperiodic / Periodic
- Avoids need of polling
- In RTX, with timer threads, ~instead of SysTick

- Polling inversion

---

